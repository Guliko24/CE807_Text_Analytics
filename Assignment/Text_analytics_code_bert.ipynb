{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guliko24/CE807_Text_Analytics/blob/main/Assignment/Text_analytics_code_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student ID: 2323017\n",
        "\n",
        "**You student_id is your 7/8 digit faser number.**\n",
        "\n",
        "This is a sample format for CE807: Assignment . You must follow the format.\n",
        "The code will have three broad sections, and additional section, if needed,\n",
        "\n",
        "\n",
        "1.   Common Codes\n",
        "2.   Method/model 1 Specific Codes\n",
        "3.   Method/model 2 Specific Codes\n",
        "4.   Other Method/model Codes, if any\n",
        "\n",
        "**You must have `train_unsup`, `test_unsup` for Unsupervised method  and `train_dis`, `test_dis` for Discriminatuve method to perform full training and testing. This will be evaluated automatically, without this your code will fail and no marked.**\n",
        "\n",
        "You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices.\n",
        "\n",
        "Before each `code block/function`, you must have a `text block` which explain what code block/function is going to do. For each function/class, you need to properly document what are it's input, functionality and output.\n",
        "\n",
        "If you are using any non-standard library, you must have command to install that, for example `pip install datasets`.\n",
        "\n",
        "You must print `train`, `validation` and `test` performance measures.\n",
        "\n",
        "You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.\n",
        "\n",
        "Your code must\n",
        "\n",
        "*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**.\n",
        "*   read dataset from './data/number/', where number is last digit of your student_id folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]\n",
        "*   save model after finishing the training in './model/student_id/Model_Unsup/' and './model/student_id/Model_Dis/' for Unsupervised and Discriminative model respectively.\n",
        "*   at testing time you will load models from './model/student_id/Model_Unsup/' and './model/student_id/Model_Dis/'  for Unsupervised and Discriminative model respectively. Your output file based on the test file will be named “test.csv” and you will add/modify “out_label_model_unsup” and “out_label_model_dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.\n",
        "*  after testing, your output file will be named “test.csv” and you will add/modify “out_label_model_unsup” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Install and import all required libraries first before starting to code.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VgzEm1gDYBUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install all require libraries. For example, `transformers`"
      ],
      "metadata": {
        "id": "_3ZWJlO6JOqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "81A_4_dKJV4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03cd643-4433-4dcb-8cff-41286c3a287e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import all require libraries.\n",
        "For example, `numpy`"
      ],
      "metadata": {
        "id": "U5XEt6asIi3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TKEZRYhIImbg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's put your student id as a variable, that you will use different places**"
      ],
      "metadata": {
        "id": "pd5kSsAPZoE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_id = 2323017 # this is my student ID corresponding to hmamay"
      ],
      "metadata": {
        "id": "rqP6pp_3ZkVy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"
      ],
      "metadata": {
        "id": "RiLUrQ-3zC6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set same seeds for all libraries\n",
        "#numpy seed\n",
        "np.random.seed(student_id)"
      ],
      "metadata": {
        "id": "TYUn2tj3zCFq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_unsup()\n",
        "test_unsup()\n",
        "train_dis()\n",
        "test_dis()"
      ],
      "metadata": {
        "id": "HsbefEImzAUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Codes\n",
        "\n",
        "In this section you will write all common codes, for examples\n",
        "\n",
        "\n",
        "*   Data read\n",
        "*   Command Line argument reading\n",
        "*   Performance Matrics\n",
        "*   Print Dataset Statistics\n",
        "*   Saving model and output\n",
        "*   Loading Model and output\n",
        "*   etc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dlj_VQrkbLgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's first allow the GDrive access and set data and model paths**\n",
        "\n",
        "For examples,\n",
        "\n",
        "student_id = 12345670\n",
        "\n",
        "set GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ‘./CE807-24-SP/Assignment/’ in your GDrive\n",
        "\n",
        "now set all global variable,\n",
        "\n",
        "\n",
        "Sample output directory and file structure: https://drive.google.com/drive/folders/1ZCVOBjsxu3bnXRk8tUVkL97Bm1MmS_gE?usp=sharing   "
      ],
      "metadata": {
        "id": "uOESFmIPn_nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "L1kvIe1NbDoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7313a222-eed5-4442-8f56-330802a347e9"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code to initialize GDrive and data and models paths\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '/content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))"
      ],
      "metadata": {
        "id": "8_vXkfWmi9mA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb38d415-ee1e-4c02-b0d2-44f89d26855a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List files:  ['data', 'model', '.ipynb_checkpoints', '__pycache__', 'sentiment_analysis.py', 'Text_analytics_code.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up model paths\n",
        "MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model')\n",
        "MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'model_dis')  # Model Discriminative directory\n",
        "MODEL_Unsup_DIRECTORY = os.path.join(MODEL_PATH, 'model_unsup')  # Model Unsupervised directory\n",
        "print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)\n",
        "print('Model Unsupervised directory: ', MODEL_Unsup_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zfnVsceCtF5",
        "outputId": "69169182-8587-427c-9258-75094679f3e8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Discriminative directory:  /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/model/model_dis\n",
            "Model Unsupervised directory:  /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/model/model_unsup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '37') # 37 is a dataset allocated to my StudetnID\n",
        "train_file = os.path.join(DATA_PATH, 'train.csv')\n",
        "print('Train file: ', train_file)\n",
        "\n",
        "val_file = os.path.join(DATA_PATH, 'valid.csv')\n",
        "print('Validation file: ', val_file)\n",
        "\n",
        "test_file = os.path.join(DATA_PATH, 'test.csv')\n",
        "print('Test file: ', test_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogH0pxjJznUb",
        "outputId": "50646b05-af55-4c55-c350-18873bcbdd2f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train file:  /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/data/37/train.csv\n",
            "Validation file:  /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/data/37/valid.csv\n",
            "Test file:  /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/data/37/test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read train Data\n",
        "train_df = pd.read_csv(train_file)\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "V7EDx2hO0AD0",
        "outputId": "21c33c04-e65f-43d8-e2db-f65185025aed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentiment                                               text\n",
              "0  positive                                        Works great\n",
              "1  negative  I drive an SUV and these do not fit on my back...\n",
              "2  positive                    It does what is supposed to be.\n",
              "3  positive  I use this on my 2006 bmw m5.  works like a ch...\n",
              "4  positive  …wasn’t what I thought it could do… thinking t..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16924327-76dc-4ba2-bd2e-928258c50ac6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>Works great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>I drive an SUV and these do not fit on my back...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>It does what is supposed to be.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>I use this on my 2006 bmw m5.  works like a ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>…wasn’t what I thought it could do… thinking t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16924327-76dc-4ba2-bd2e-928258c50ac6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-16924327-76dc-4ba2-bd2e-928258c50ac6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-16924327-76dc-4ba2-bd2e-928258c50ac6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed56fa82-176e-43ab-aecc-1fcdffea6996\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed56fa82-176e-43ab-aecc-1fcdffea6996')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed56fa82-176e-43ab-aecc-1fcdffea6996 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 3681,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3543,\n        \"samples\": [\n          \"It is what it is\",\n          \"Love the sound but muffler was dented and it was a pain in the a$& to install because these parts are not spot on as far as fitment. Amazon would only refund 5% for the damage which really sucks since I paid over $1000 for this thing!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q5uMdw85Spu",
        "outputId": "49296846-82a3-474f-9b83-b4559e5a4f9a"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3681, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Validation Data\n",
        "val_df = pd.read_csv(val_file)\n",
        "val_df.head()\n",
        "val_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0iekwoY0Tsy",
        "outputId": "e94b263a-da9c-4566-b5e1-6cafbe8f3c7c"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(454, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Test Data\n",
        "test_df = pd.read_csv(test_file)\n",
        "test_df.head()\n",
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnaBbn2P0Z5E",
        "outputId": "efa08aa1-b5d9-47cb-ed62-f6b1e58691ba"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(409, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's update the environment to be able to call functions saved in .py\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XjE4mpEOdk6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "metadata": {
        "id": "5y9Kw_Gcd3hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56d1790-8dd1-490d-9a5a-be90e8aa8aa3"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "from sentiment_analysis import preprocess_text\n",
        "from sentiment_analysis import encode_sentiment"
      ],
      "metadata": {
        "id": "OtEcor4Md-OQ"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_name):\n",
        "  df = pd.read_csv(file_name)\n",
        "  print(file_name, 'has', len(df),'data points')\n",
        "  return df"
      ],
      "metadata": {
        "id": "QFAP8TeT6yrD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's modify all the data sets by removing empty rows and encoding the tartget_column='sentiment'\n",
        "# Remove rows with NaNs in both 'text' and 'label' columns\n",
        "train_df = train_df.dropna(subset=['text', 'sentiment'])\n",
        "val_df = val_df.dropna(subset=['text', 'sentiment'])\n",
        "\n",
        "#encode the sentiments\n",
        "train_df['sentiment_encoded'] = train_df['sentiment'].map({'negative': 0, 'positive': 1})\n",
        "val_df['sentiment_encoded'] = val_df['sentiment'].map({'negative': 0, 'positive': 1})\n"
      ],
      "metadata": {
        "id": "3XmEL3qo7qY1"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CCuxmxb_d3N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "khsX3ZkuIlOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method Unsupervised Start\n",
        "\n",
        "In this section you will write all details of your Method 1.\n",
        "\n",
        "You will have to enter multiple `code` and `text` cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "\n",
        "*   Data reading\n",
        "*   Data clearning, if any\n",
        "*   Convert data to vector/tokenization/vectorization\n",
        "*   Model Declaration/Initialization/building\n",
        "*   Training and validation of the model using training and validation dataset\n",
        "*   Save the trained model\n",
        "*   Load and Test the model on testing set\n",
        "*   Save the output of the model\n",
        "\n",
        "\n",
        "You could add any other step(s) based on your method's requirement.\n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"
      ],
      "metadata": {
        "id": "47ywe8jGSKhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code(s)"
      ],
      "metadata": {
        "id": "6W5H-VeKi7-z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Unsupervised Method Code\n",
        "Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n",
        "\n",
        "Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best."
      ],
      "metadata": {
        "id": "1sA3OWlVbnoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code(s)"
      ],
      "metadata": {
        "id": "fkJ5LiXoipcX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Method Unsupervised Code\n",
        "Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "
      ],
      "metadata": {
        "id": "qyJ_xv12Uy9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code(s)"
      ],
      "metadata": {
        "id": "UZs5nzA4ivqT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method Unsupervised End\n"
      ],
      "metadata": {
        "id": "ue3xIDFGSXNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method Discriminative Start\n",
        "\n",
        "In this section you will write all details of your Method 2.\n",
        "\n",
        "You will have to enter multiple `code` and `text` cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "\n",
        "*   Data reading\n",
        "*   Data clearning, if any\n",
        "*   Convert data to vector/tokenization/vectorization\n",
        "*   Model Declaration/Initialization/building\n",
        "*   Training and validation of the model using training and validation dataset\n",
        "*   Save the trained model\n",
        "*   Load and Test the model on testing set\n",
        "*   Save the output of the model\n",
        "\n",
        "You could add any other step(s) based on your method's requirement.\n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"
      ],
      "metadata": {
        "id": "o5jNIHneSfzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code(s)"
      ],
      "metadata": {
        "id": "KZzeItA7iyIj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Method Discriminative Code\n",
        "Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n",
        "\n",
        "Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best."
      ],
      "metadata": {
        "id": "zAkC0CWAc1BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n"
      ],
      "metadata": {
        "id": "Ct7XBqqe1WAQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dis(train_df, val_df, model_dir):\n",
        "    \"\"\"\n",
        "    Trains a text classifier using SVM on the provided training DataFrame and validates on the validation DataFrame.\n",
        "    Saves the best model and vocab in the model_dir.\n",
        "\n",
        "    Args:\n",
        "        train_df: DataFrame containing training data with 'text' and 'sentiment' columns\n",
        "        val_df: DataFrame containing validation data with 'text' and 'sentiment' columns\n",
        "        model_dir: Directory to save the trained model and vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Training set has', len(train_df),'data points')\n",
        "    print('Validation set has', len(val_df),'data points')\n",
        "\n",
        "    # Create vocabulary using count vectorizer\n",
        "    count_vectorizer = CountVectorizer() # Note no pre-processing is done here. In practice you will have different preprocessing steps.\n",
        "    counts = count_vectorizer.fit_transform(train_data['text'].values)\n",
        "\n",
        "    # NB classifier\n",
        "    classifier = MultinomialNB()\n",
        "    targets = train_data['label'].values\n",
        "\n",
        "    #train the classifier\n",
        "    print('Start training')\n",
        "    classifier.fit(counts, targets)\n",
        "    print('Done training')\n",
        "\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(classifier, open(model_file, 'wb'))\n",
        "    print('Saved model to', model_file)\n",
        "\n",
        "    vocab_file = os.path.join(model_dir, 'vocab.sav')\n",
        "    pickle.dump(count_vectorizer, open(vocab_file, 'wb'))\n",
        "    print('Saved vocab to', vocab_file)\n",
        "\n",
        "    # Testing on the validation data\n",
        "    val_counts=count_vectorizer.transform(val_df['text'])\n",
        "    predictions= classifier.predict(val_counts)\n",
        "    print('Validation Done')\n",
        "    target  = val_df['label']\n",
        "\n",
        "    score=f1_score(target, predictions, average='macro')\n",
        "\n",
        "    print('macro F1 Score on Validation set', score)\n",
        "    ##########################################################################\n",
        "    #                            END OF YOUR CODE                            #\n",
        "    ##########################################################################\n",
        "    return"
      ],
      "metadata": {
        "id": "UAfRiq7cixij"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Gen(train_file, val_file, MODEL_Dis_DIRECTORY)"
      ],
      "metadata": {
        "id": "xwbkavf81mgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "b6ec1a22-0ae2-46ef-e89b-e6414f0179ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 3681 data points\n",
            "Validation set has 454 data points\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-cf829f43d5ed>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_Gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_Dis_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-5221fbe0481a>\u001b[0m in \u001b[0;36mtrain_Gen\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# NB classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_Gen(test_file, model_dir):\n",
        "    \"\"\"\n",
        "    Takes train_file, val_file and model_dir as input.\n",
        "    It trained on the train_file datapoints, and validate on the val_file datapoints.\n",
        "    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n",
        "    After finishing the training, it saved the best model in the model_dir.\n",
        "\n",
        "    ADD Other arguments, if needed.\n",
        "\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "    # Read training and validation set data\n",
        "    # test_df = pd.read_csv(test_file)\n",
        "    # print('Testing set has', len(test_df),'data points')\n",
        "\n",
        "    test_df = read_data(test_file)\n",
        "\n",
        "    model_file = model_dir +'/model.sav'\n",
        "    classifier = pickle.load(open(model_file, 'rb'))\n",
        "    print('Model loaded')\n",
        "\n",
        "    vocab_file = model_dir +'/vocab.sav'\n",
        "    count_vectorizer = pickle.load(open(vocab_file, 'rb'))\n",
        "    print('Vocab loaded')\n",
        "\n",
        "    # Testing on the test data\n",
        "    test_counts=count_vectorizer.transform(test_df['text'])\n",
        "    predictions= classifier.predict(test_counts)\n",
        "    print('Testing Done')\n",
        "    target  = test_df['label']\n",
        "\n",
        "    score=f1_score(target, predictions, average='macro')\n",
        "\n",
        "    print('macro F1 Score on Test set', score)\n",
        "\n",
        "    test_df['out_label_model_dis'] = predictions\n",
        "\n",
        "    test_df.to_csv(test_file,index=False)\n",
        "    print('Saved output to ',test_file)\n",
        "\n",
        "    ##########################################################################\n",
        "    #                            END OF YOUR CODE                            #\n",
        "    ##########################################################################\n"
      ],
      "metadata": {
        "id": "AdNsN_-L4vIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_Gen(test_file, MODEL_Dis_DIRECTORY)"
      ],
      "metadata": {
        "id": "X1IZLmDH6F-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Method Discriminative Code\n",
        "Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "
      ],
      "metadata": {
        "id": "LVi2vGeZc5Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code(s)"
      ],
      "metadata": {
        "id": "VSTzjETEiy_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminative Method  End\n"
      ],
      "metadata": {
        "id": "4gNwyxXNSmVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Method/model Start"
      ],
      "metadata": {
        "id": "rmaJfJkVwSDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train 2 models, starting with SVM then proceeding to BERT. Both are discriminative models. After assessing the evaluation of the models, I will decide which one to choose to work with the test data."
      ],
      "metadata": {
        "id": "rAM5g3XJtkxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discriminative model SVM**"
      ],
      "metadata": {
        "id": "AdbayX22t_QS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I ran the model on raw/unprocessed data and it failed to train. So let's pre-proces and modify the code and evaluate."
      ],
      "metadata": {
        "id": "a8nVewBqw2Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "model_dir=MODEL_Dis_DIRECTORY\n",
        "def train_dis(train_df, val_df, model_dir):\n",
        "    \"\"\"\n",
        "    Trains a text classifier using SVM on the provided training DataFrame and validates on the validation DataFrame.\n",
        "    Saves the best model and vocab in the model_dir.\n",
        "\n",
        "    Args:\n",
        "        train_df: DataFrame containing training data with 'text', 'sentiment' and 'encoded_sentiment' columns\n",
        "        val_df: DataFrame containing validation data with 'text', 'sentiment' and 'encoded_sentiment' columns\n",
        "        model_dir: Directory to save the trained model and vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    #encode the sentiments\n",
        "    train_df['sentiment_encoded'] = train_df['sentiment'].map({'negative': 0, 'positive': 1})\n",
        "    val_df['sentiment_encoded'] = val_df['sentiment'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "    # Remove rows with NaNs in both 'text' and 'label' columns\n",
        "    train_df = train_df.dropna(subset=['text', 'sentiment'])\n",
        "    val_df = val_df.dropna(subset=['text', 'sentiment'])\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "    val_df['text'] = val_df['text'].apply(preprocess_text)\n",
        "\n",
        "   # Check if preprocessed texts are empty\n",
        "    train_df = train_df[train_df['text'].str.strip() != '']\n",
        "    val_df = val_df[val_df['text'].str.strip() != '']\n",
        "\n",
        "    if train_df.empty:\n",
        "        raise ValueError(\"Training data is empty after preprocessing. Adjust preprocessing steps.\")\n",
        "\n",
        "    if val_df.empty:\n",
        "        raise ValueError(\"Validation data is empty after preprocessing. Adjust preprocessing steps.\")\n",
        "\n",
        "    print('Training set has', len(train_df), 'data points')\n",
        "    print('Validation set has', len(val_df), 'data points')\n",
        "\n",
        "    # Create vocabulary using TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    counts = tfidf_vectorizer.fit_transform(train_df['text'].values)\n",
        "    targets = train_df['sentiment_encoded'].values\n",
        "    print(f\"Feature matrix shape: {counts.shape}\")\n",
        "    print(f\"Number of targets: {len(targets)}\")\n",
        "    if counts.shape[0] != len(targets):\n",
        "        raise ValueError(f\"Feature matrix has {counts.shape[0]} samples, but target has {len(targets)} samples. Check preprocessing steps.\")\n",
        "\n",
        "\n",
        "    # Apply SMOTE to balance the classes\n",
        "    smote = SMOTE(random_state=student_id)\n",
        "    counts_resampled, targets_resampled = smote.fit_resample(counts, train_df['sentiment_encoded'].values)\n",
        "\n",
        "    # Ensure SMOTE output is valid\n",
        "    print(f\"SMOTE feature matrix shape: {counts_resampled.shape}\")\n",
        "    print(f\"Number of SMOTE targets: {len(targets_resampled)}\")\n",
        "\n",
        "\n",
        "    # SVM classifier\n",
        "    classifier = SVC(kernel='linear', probability=True)  # You can choose other kernels like 'rbf', 'poly', etc.\n",
        "    targets = train_df['sentiment_encoded'].values\n",
        "\n",
        "    # Train the classifier\n",
        "    print('Start training')\n",
        "    classifier.fit(counts_resampled, targets_resampled)\n",
        "    print('Done training')\n",
        "\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(classifier, open(model_file, 'wb'))\n",
        "    print('Saved model to', model_file)\n",
        "\n",
        "    vocab_file = os.path.join(model_dir, 'vocab.sav')\n",
        "    pickle.dump(tfidf_vectorizer, open(vocab_file, 'wb'))\n",
        "    print('Saved vocab to', vocab_file)\n",
        "\n",
        "    # Testing on the validation data\n",
        "    #val_counts = tfidf_vectorizer.transform(val_df['text'])\n",
        "    #predictions = classifier.predict(val_counts)\n",
        "    #print('Validation Done')\n",
        "    #target = val_df['sentiment_encoded']\n",
        "\n",
        "    #score = f1_score(target, predictions, average='macro')\n",
        "\n",
        "    #print('Macro F1 Score on Validation set:', score)\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "V1BL_w7Ai0pa"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, labels):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "K-KwOXCPYlkQ"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir=MODEL_Dis_DIRECTORY\n",
        "\n",
        "def test_model(val_df, model_dir):\n",
        "    \"\"\"\n",
        "    Loads the saved model and vocabulary, then tests it on the validation data.\n",
        "\n",
        "    Args:\n",
        "        val_df: Validation dataframe define earliere\n",
        "        model_dir: Model directory where the model and vocabulary are saved\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "     #encode the sentiments\n",
        "    #train_df['sentiment_encoded'] = train_df['sentiment'].map({'negative': 0, 'positive': 1})\n",
        "   # val_df['sentiment_encoded'] = val_df['sentiment'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "    # Remove rows with NaNs in both 'text' and 'label' columns\n",
        "    #train_df = train_df.dropna(subset=['text', 'sentiment'])\n",
        "    #val_df = val_df.dropna(subset=['text', 'sentiment'])\n",
        "\n",
        "    # Preprocess the text data\n",
        "    val_df['text'] = val_df['text'].apply(preprocess_text)\n",
        "\n",
        "    # Check if preprocessed texts are empty\n",
        "    val_df = val_df[val_df['text'].str.strip() != '']\n",
        "\n",
        "    if val_df.empty:\n",
        "        raise ValueError(\"Validation data is empty after preprocessing. Adjust preprocessing steps.\")\n",
        "\n",
        "\n",
        "    # Load the saved model and vocabulary\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    vocab_file = os.path.join(model_dir, 'vocab.sav')\n",
        "\n",
        "    classifier = pickle.load(open(model_file, 'rb'))\n",
        "    tfidf_vectorizer = pickle.load(open(vocab_file, 'rb'))\n",
        "    print('Loaded model and vocabulary from', model_dir)\n",
        "\n",
        "    # Testing on the validation data\n",
        "    val_counts = tfidf_vectorizer.transform(val_df['text'])\n",
        "    predictions = classifier.predict(val_counts)\n",
        "    target = val_df['sentiment_encoded']\n",
        "\n",
        "   # Compute confusion matrix\n",
        "    cm = confusion_matrix(target, predictions)\n",
        "    class_names = [str(cls) for cls in classifier.classes_]\n",
        "    plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "    # Print classification report\n",
        "    report = classification_report(target, predictions, target_names=class_names)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Calculate and print Macro F1 Score\n",
        "    score = f1_score(target, predictions, average='macro')\n",
        "    print('Macro F1 Score on Validation set:', score)\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "6Ht7rklMvExV"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's now train and test our SVM model on the data we have and save the model and then validate it\n",
        "#on the validation file\n",
        "train_dis(train_df, val_df, model_dir )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay8zxM1pveKI",
        "outputId": "c829c7d3-92dd-4f67-a52a-317009140753"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 3666 data points\n",
            "Validation set has 454 data points\n",
            "Feature matrix shape: (3666, 8240)\n",
            "Number of targets: 3666\n",
            "SMOTE feature matrix shape: (6034, 8240)\n",
            "Number of SMOTE targets: 6034\n",
            "Start training\n",
            "Done training\n",
            "Saved model to /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/model/model_dis/model.sav\n",
            "Saved vocab to /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/model/model_dis/vocab.sav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize and test the saved model on the validation set, i.e fine tune it\n",
        "test_model(val_df, model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "58uJgqBJVdJ2",
        "outputId": "04920253-b743-4ec4-8f27-d8f833b8f749"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model and vocabulary from /content/gdrive/MyDrive/CE807-24-SU/Assignment_StudentID_2323017/model/model_dis\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAAJaCAYAAABQj8p9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4XUlEQVR4nO3de5RWZd0//vcMh5EzjgoDKYqiIo9mhIbTwSOJSiiJlWWGZfpooCmewtTMyjHLLA9IponfjMosqSg1woAsPERaZkppFvroAIqCgzKc5vdHv+a55/Fws3WYGez1cu21mL33vfdnNmuNfOZ9XfuqaGpqagoAAMBGqmzvAgAAgM2LJgIAAChEEwEAABSiiQAAAArRRAAAAIVoIgAAgEI0EQAAQCGaCAAAoBBNBAAAUEjn9i5gU1i+an17lwDQqrpXdWrvEgBa1RYd+F+h3YZParN7vXT/VW12r9YkiQAAAArpwD0gAAC0gwq/Zy/HEwIAAAqRRAAAQKmKivauoMOTRAAAAIVIIgAAoJQ5EWV5QgAAQCGSCAAAKGVORFmSCAAAoBBJBAAAlDInoixPCAAAKEQSAQAApcyJKEsSAQAAFCKJAACAUuZElOUJAQAAhWgiAACAQgxnAgCAUiZWlyWJAAAACpFEAABAKROry/KEAACAQiQRAABQypyIsiQRAABAIZIIAAAoZU5EWZ4QAABQiCQCAABKmRNRliQCAAAoRBIBAAClzIkoyxMCAAAKkUQAAEApSURZnhAAAFCIJAIAAEpVejtTOZIIAACgEEkEAACUMieiLE8IAAAoRBMBAAAUYjgTAACUqjCxuhxJBAAAUIgkAgAASplYXZYnBAAAFCKJAACAUuZElCWJAAAACpFEAABAKXMiyvKEAACAQiQRAABQypyIsiQRAABAIZIIAAAoZU5EWZ4QAABQiCQCAABKmRNRliQCAAAoRBIBAAClzIkoyxMCAAAKkUQAAEApcyLKkkQAAMBm4Jprrslb3/rW9O7dO717905tbW1uu+225uOrV6/OxIkTs9VWW6Vnz54ZP358lixZ0uIaixcvzpgxY9K9e/f069cvZ511VtatW1e4Fk0EAACUqqhsu62AbbfdNpdcckkWLlyY3//+9znwwANzxBFH5KGHHkqSnH766fnZz36WH/7wh5k3b16eeuqpHHnkkc2fX79+fcaMGZM1a9bkd7/7XW688cZMnz49F1xwQfFH1NTU1FT4Ux3c8lXr27sEgFbVvapTe5cA0Kq26MCD6ru976o2u9dLsya9oc9XV1fnK1/5So466qhss802mTFjRo466qgkySOPPJLddtstCxYsyD777JPbbrst73vf+/LUU0+lf//+SZJp06blnHPOybJly9K1a9eNvq8kAgAA2kljY2NWrlzZYmtsbCz7ufXr1+f73/9+Vq1aldra2ixcuDBr167NqFGjms8ZOnRoBg0alAULFiRJFixYkD322KO5gUiS0aNHZ+XKlc1pxsbSRAAAQKk2HM5UV1eXPn36tNjq6upetbQHH3wwPXv2TFVVVU466aTceuutGTZsWOrr69O1a9f07du3xfn9+/dPfX19kqS+vr5FA/Hv4/8+VkQHDpIAAODNbcqUKZk8eXKLfVVVVa96/q677poHHnggK1asyC233JIJEyZk3rx5m7rMl9FEAABAqTZ8xWtVVdVrNg3/V9euXTNkyJAkyYgRI3LfffflG9/4Rj70oQ9lzZo1ef7551ukEUuWLElNTU2SpKamJvfee2+L6/377U3/PmdjGc4EAACbqQ0bNqSxsTEjRoxIly5dMmfOnOZjixYtyuLFi1NbW5skqa2tzYMPPpilS5c2nzN79uz07t07w4YNK3RfSQQAAJQq+OrVtjJlypQceuihGTRoUF544YXMmDEjc+fOzR133JE+ffrk+OOPz+TJk1NdXZ3evXvnlFNOSW1tbfbZZ58kycEHH5xhw4bl2GOPzaWXXpr6+vqcd955mThxYqE0JNFEAADAZmHp0qX52Mc+lqeffjp9+vTJW9/61txxxx1573vfmyS5/PLLU1lZmfHjx6exsTGjR4/O1KlTmz/fqVOnzJo1KyeffHJqa2vTo0ePTJgwIRdddFHhWqwTAbAZsE4E8GbTodeJGHdtm93rpZknttm9WlPHzGoAAIAOqwP3gAAA0A466JyIjsQTAgAACpFEAABAqTZcJ2JzJYkAAAAKkUQAAECJCklEWZIIAACgEEkEAACUkESUJ4kAAAAKkUQAAEApQURZkggAAKAQTQQAAFCI4UwAAFDCxOryJBEAAEAhkggAACghiShPEgEAABQiiQAAgBKSiPIkEQAAQCGSCAAAKCGJKE8SAQAAFCKJAACAUoKIsiQRAABAIZIIAAAoYU5EeZIIAACgEEkEAACUkESUJ4kAAAAKkUQAAEAJSUR5kggAAKAQSQQAAJSQRJQniQAAAAqRRAAAQClBRFmSCAAAoBBNBAAAUIjhTAAAUMLE6vIkEQAAQCGSCAAAKCGJKE8SAQAAFCKJAACAEpKI8iQRAABAIZIIAAAoJYgoSxIBAAAUIokAAIAS5kSUJ4kAAAAKkUQAAEAJSUR5kggAAKAQSQQAAJSQRJQniQAAAAqRRAAAQAlJRHmSCAAAoBBJBAAAlBJElCWJAAAACtFEAAAAhRjOBAAAJUysLk8SAQAAFCKJAACAEpKI8iQRAABAIZIIAAAoIYkoTxIBAAAUIokAAIBSgoiyJBEAAEAhkggAAChhTkR5kggAAKAQSQQAAJSQRJQniQAAAAqRRAAAQAlJRHmSCCjg/93wrdS+fVgu/0pd875nn1mWz593Tsa89z054J0jMuEj4/PrOb9sxyoBXtvN35+Ro94/Nu98x9vzzne8Pcd+5EO56zfzkiQrnn8+dV/6Qg4fMzrvePtbM/qg/XPJxV/MCy+80M5VAx2JJAI20l8eejAzf3Rzhuy8a4v9F10wJS+88EIuvfzq9O27ZX55+89z3jmT8+2bbs6uQ4e1U7UAr65f/5p8+vQzM2j77dPU1JSf/WRmPj1pYn7wo1vT1NSUZUuXZvKZ52SnnYbkqaf+J1+86MIsW7o0l339ivYuHdqEJKI8SQRshBdfXJULP3t2PnP+59Ord+8Wxx784/35wIeOyX/t/ta8Zdvt8vFPnpSevXpl0cN/aadqAV7b/gccmPfsu1+2336H7LDD4Jzy6dPTvXv3/OmPD2TnnXfJ175xZfY/4MBsN2hQRu5Tm1M+fVrmzb0z69ata+/SgQ6iXZOIZ555Jt/+9rezYMGC1NfXJ0lqamryzne+M8cdd1y22Wab9iwPmn31ki/mne/eL+8Y+c5Mv+6bLY7tsefw/OqXt+Wd79k3vXr1zpzZt2dN45oMH7F3O1ULsPHWr1+fX95xe1566cXsuefwVzyn4YWG9OzZM507G8DAfwhBRFnt9tPgvvvuy+jRo9O9e/eMGjUqu+yyS5JkyZIlueKKK3LJJZfkjjvuyF577fWa12lsbExjY2PLfes6p6qqapPVzn+W2Xf8Iose+Uu+/Z2bX/H4F7/8tZx/zhk55IB3plPnztliiy1yyWVXZLtB27dxpQAb729/XZRjP3J01qxpTPfu3XP5FVdnpyFDXnbec88tz7XTpmb8Bz7UDlUCHVW7NRGnnHJKPvCBD2TatGkvG3fW1NSUk046KaecckoWLFjwmtepq6vL5z//+Rb7zp5yfs757OdavWb+8yypfzqXf6UuV0y97lUb02unXpEXGlbmimuuT98tt8z8X8/JeedMzjXXfydDdt6ljSsG2Dg77DA4N/9oZhoaXsjsX96R8889J9dPv6lFI9HQ0JBJJ/93dtxpp5z0qUntWC20LXMiyqtoampqao8bd+vWLffff3+GDh36iscfeeSRDB8+PC+99NJrXueVkohVkghaybxf/yqfOePUdOrUqXnf+vXrU1FRkcrKynz/xz/PB444JN/94U+y4047N59zykmfyLbbDco5n72wHarmzah7VafyJ8EbcOLxx2Xb7QblggsvSpKsWtWQk0/8ZLbYYotcOfWb/r9Kq9uiA4+O23HyL9rsXn//2mFtdq/W1G5/fTU1Nbn33ntftYm49957079//7LXqaqqetkPtnWr1rdKjbDXO2pz080/abHvSxd+NtvvMDgfPe6TWb16dZKksqLlOwo6VXZK04Z26c8BXpcNGzZk7Zo1Sf6VQJx84vHp2rVrvnHVNRoI4GXarYk488wzc+KJJ2bhwoU56KCDmhuGJUuWZM6cOfnWt76Vr371q+1VHiRJevTokZ2G7Nxi3xbduqV3n77ZacjOWbd2bbbdblC+/KULM+n0s9KnT9/Mnzsn997zu3z1G1PbqWqA1/aNyy/Lu9+zb2oGDMiLq1blFz+fld/fd2+uufb6NDQ05KQTPpHVq1/KxZd8JasaGrKqoSFJsmV1dYtkFt6sDGcqr92aiIkTJ2brrbfO5ZdfnqlTp2b9+n+lB506dcqIESMyffr0fPCDH2yv8mCjdO7SJV+7clqmXnF5zjptYl568cVsu92gnP/5urzz3fu1d3kAr2j58mdz3pRzsmzZ0vTs1Su77LJrrrn2+tS+812579578uCf/pgked+h723xuV/8ck7e8pZt26NkoINptzkRpdauXZtnnnkmSbL11lunS5cub+h6yw1nAt5kzIkA3mw68pyIIWfe1mb3evSrh7bZvVpTh/jr69KlSwYMGNDeZQAAABuhQzQRAADQUZgTUV5l+VMAAAD+lyYCAABKVFS03VZEXV1d9t577/Tq1Sv9+vXLuHHjsmjRohbn7L///qmoqGixnXTSSS3OWbx4ccaMGZPu3bunX79+Oeuss7Ju3bpCtRjOBAAAm4F58+Zl4sSJ2XvvvbNu3bqce+65Ofjgg/OXv/wlPXr0aD7vhBNOyEUXXdT8dffu3Zv/vH79+owZMyY1NTX53e9+l6effjof+9jH0qVLl1x88cUbXYsmAgAASnTUORG33357i6+nT5+efv36ZeHChdl3332b93fv3j01NTWveI1f/vKX+ctf/pJf/epX6d+/f972trflC1/4Qs4555xceOGF6dq160bVYjgTAAC0k8bGxqxcubLF1tjYuFGfXbFiRZKkurq6xf7vfve72XrrrbP77rtnypQpefHFF5uPLViwIHvssUfzQs9JMnr06KxcuTIPPfTQRtetiQAAgBJtOSeirq4uffr0abHV1dWVrXHDhg057bTT8q53vSu777578/6PfOQjuemmm/LrX/86U6ZMyXe+85189KMfbT5eX1/fooFI0vx1fX39Rj8jw5kAAKCdTJkyJZMnT26xr6qqquznJk6cmD//+c+56667Wuw/8cQTm/+8xx57ZMCAATnooIPy2GOPZaeddmqdoqOJAACAFior225ORFVV1UY1DaUmTZqUWbNmZf78+dl2221f89yRI0cmSR599NHstNNOqampyb333tvinCVLliTJq86jeCWGMwEAwGagqakpkyZNyq233po777wzgwcPLvuZBx54IEkyYMCAJEltbW0efPDBLF26tPmc2bNnp3fv3hk2bNhG1yKJAACAEh305UyZOHFiZsyYkZ/85Cfp1atX8xyGPn36pFu3bnnssccyY8aMHHbYYdlqq63ypz/9Kaeffnr23XffvPWtb02SHHzwwRk2bFiOPfbYXHrppamvr895552XiRMnFkpEKpqampo2yXfZjpavWt/eJQC0qu5Vndq7BIBWtUUH/lX2f332l212r4e+dPBGn/tqr5694YYbctxxx+WJJ57IRz/60fz5z3/OqlWrst122+X9739/zjvvvPTu3bv5/H/+8585+eSTM3fu3PTo0SMTJkzIJZdcks6dN/4vRRMBsBnQRABvNh25idj9vNltdq8/f/G9bXav1mROBAAAUIgmAgAAKKQDB0kAAND2OurE6o5EEgEAABQiiQAAgBKv9hYk/pckAgAAKEQSAQAAJSQR5UkiAACAQiQRAABQQhBRniQCAAAoRBIBAAAlzIkoTxIBAAAUIokAAIASgojyJBEAAEAhkggAAChhTkR5kggAAKAQSQQAAJQQRJQniQAAAAqRRAAAQAlzIsqTRAAAAIVIIgAAoIQgojxJBAAAUIgmAgAAKMRwJgAAKGFidXmSCAAAoBBJBAAAlBBElCeJAAAACpFEAABACXMiypNEAAAAhUgiAACghCCiPEkEAABQiCQCAABKmBNRniQCAAAoRBIBAAAlBBHlSSIAAIBCJBEAAFDCnIjyJBEAAEAhkggAACghiShPEgEAABQiiQAAgBKCiPIkEQAAQCGaCAAAoBDDmQAAoISJ1eVJIgAAgEIkEQAAUEIQUZ4kAgAAKEQSAQAAJcyJKE8SAQAAFCKJAACAEoKI8iQRAABAIZIIAAAoUSmKKEsSAQAAFCKJAACAEoKI8iQRAABAIZIIAAAoYZ2I8iQRAABAIZIIAAAoUSmIKEsSAQAAFCKJAACAEuZElCeJAAAACpFEAABACUFEeZIIAACgEE0EAABQiOFMAABQoiLGM5UjiQAAAAqRRAAAQAmLzZUniQAAAAqRRAAAQAmLzZUniQAAAAqRRAAAQAlBRHmSCAAAoBBJBAAAlKgURZQliQAAAAqRRAAAQAlBRHmSCAAAoBBJBAAAlLBORHmSCAAAoBBJBAAAlBBElCeJAAAACtFEAABAicqKijbbiqirq8vee++dXr16pV+/fhk3blwWLVrU4pzVq1dn4sSJ2WqrrdKzZ8+MHz8+S5YsaXHO4sWLM2bMmHTv3j39+vXLWWedlXXr1hV7RoXOBgAA2sW8efMyceLE3H333Zk9e3bWrl2bgw8+OKtWrWo+5/TTT8/Pfvaz/PCHP8y8efPy1FNP5cgjj2w+vn79+owZMyZr1qzJ7373u9x4442ZPn16LrjggkK1VDQ1NTW12nfWQSxftb69SwBoVd2rOrV3CQCtaosOPDP3Qzfe32b3+sGE4a/7s8uWLUu/fv0yb9687LvvvlmxYkW22WabzJgxI0cddVSS5JFHHsluu+2WBQsWZJ999sltt92W973vfXnqqafSv3//JMm0adNyzjnnZNmyZenatetG3VsSAQAAJSracGtsbMzKlStbbI2NjRtV54oVK5Ik1dXVSZKFCxdm7dq1GTVqVPM5Q4cOzaBBg7JgwYIkyYIFC7LHHns0NxBJMnr06KxcuTIPPfTQRj8jTQQAALSTurq69OnTp8VWV1dX9nMbNmzIaaedlne9613ZfffdkyT19fXp2rVr+vbt2+Lc/v37p76+vvmc0gbi38f/fWxjdeAgCQAA2l5bLjY3ZcqUTJ48ucW+qqqqsp+bOHFi/vznP+euu+7aVKW9Jk0EAAC0k6qqqo1qGkpNmjQps2bNyvz587Pttts276+pqcmaNWvy/PPPt0gjlixZkpqamuZz7r333hbX+/fbm/59zsYwnAkAAEpUVrTdVkRTU1MmTZqUW2+9NXfeeWcGDx7c4viIESPSpUuXzJkzp3nfokWLsnjx4tTW1iZJamtr8+CDD2bp0qXN58yePTu9e/fOsGHDNroWSQQAAGwGJk6cmBkzZuQnP/lJevXq1TyHoU+fPunWrVv69OmT448/PpMnT051dXV69+6dU045JbW1tdlnn32SJAcffHCGDRuWY489Npdeemnq6+tz3nnnZeLEiYUSEU0EAACUaMs5EUVcc801SZL999+/xf4bbrghxx13XJLk8ssvT2VlZcaPH5/GxsaMHj06U6dObT63U6dOmTVrVk4++eTU1tamR48emTBhQi666KJCtVgnAmAzYJ0I4M2mI68T8dGb/thm97rpo3u22b1aUwf+6wMAgLbXQYOIDsXEagAAoBBJBAAAlOiocyI6EkkEAABQiCQCAABKFF2/4T+RJAIAAChko5KIn/70pxt9wcMPP/x1FwMAAO3NnIjyNqqJGDdu3EZdrKKiIuvXW6MBAADezDaqidiwYcOmrgMAADoEOUR55kQAAACFvK63M61atSrz5s3L4sWLs2bNmhbHTj311FYpDAAA2kOlORFlFW4i7r///hx22GF58cUXs2rVqlRXV+eZZ55J9+7d069fP00EAAC8yRUeznT66adn7Nixee6559KtW7fcfffd+ec//5kRI0bkq1/96qaoEQAA6EAKNxEPPPBAzjjjjFRWVqZTp05pbGzMdtttl0svvTTnnnvupqgRAADaTEVF222bq8JNRJcuXVJZ+a+P9evXL4sXL06S9OnTJ0888UTrVgcAAHQ4hedEDB8+PPfdd1923nnn7LfffrngggvyzDPP5Dvf+U523333TVEjAAC0GYvNlVc4ibj44oszYMCAJMmXvvSlbLnlljn55JOzbNmyXHvtta1eIAAA0LEUTiL22muv5j/369cvt99+e6sWBAAA7UkQUZ7F5gAAgEIKJxGDBw9+zXFif//7399QQQAA0J4sNlde4SbitNNOa/H12rVrc//99+f222/PWWed1Vp1AQAAHVThJuLTn/70K+6/+uqr8/vf//4NFwQAAO1JEFFeq82JOPTQQ/OjH/2otS4HAAB0UIWTiFdzyy23pLq6urUuBwAA7cI6EeW9rsXmSh9sU1NT6uvrs2zZskydOrVViwMAADqewk3EEUcc0aKJqKyszDbbbJP9998/Q4cObdXiXq/OnXSPwJvLlntPau8SAFrVS/df1d4lvCprIJRXuIm48MILN0EZAADA5qJwo9WpU6csXbr0ZfufffbZdOrUqVWKAgCA9lJRUdFm2+aqcBPR1NT0ivsbGxvTtWvXN1wQAADQsW30cKYrrrgiyb86s+uuuy49e/ZsPrZ+/frMnz+/w8yJAACA16ty8w0I2sxGNxGXX355kn8lEdOmTWsxdKlr167ZYYcdMm3atNavEAAA6FA2uol4/PHHkyQHHHBAfvzjH2fLLbfcZEUBAAAdV+G3M/3617/eFHUAAECHYDhTeYUnVo8fPz5f/vKXX7b/0ksvzQc+8IFWKQoAAOi4CjcR8+fPz2GHHfay/Yceemjmz5/fKkUBAEB78YrX8go3EQ0NDa/4KtcuXbpk5cqVrVIUAADQcRVuIvbYY4/84Ac/eNn+73//+xk2bFirFAUAAO2lsqLtts1V4YnV559/fo488sg89thjOfDAA5Mkc+bMyYwZM3LLLbe0eoEAAEDHUriJGDt2bGbOnJmLL744t9xyS7p165Y999wzd955Z6qrqzdFjQAA0GY246kKbaZwE5EkY8aMyZgxY5IkK1euzPe+972ceeaZWbhwYdavX9+qBQIAAB1L4TkR/zZ//vxMmDAhAwcOzGWXXZYDDzwwd999d2vWBgAAba6yoqLNts1VoSSivr4+06dPz/XXX5+VK1fmgx/8YBobGzNz5kyTqgEA4D/ERicRY8eOza677po//elP+frXv56nnnoqV1555aasDQAA2lxlG26bq41OIm677baceuqpOfnkk7PzzjtvypoAAIAObKMboLvuuisvvPBCRowYkZEjR+aqq67KM888sylrAwCANldR0Xbb5mqjm4h99tkn3/rWt/L000/nv//7v/P9738/AwcOzIYNGzJ79uy88MILm7JOAACggyg8FKtHjx75xCc+kbvuuisPPvhgzjjjjFxyySXp169fDj/88E1RIwAAtBlvZyrvDc3n2HXXXXPppZfmySefzPe+973WqgkAAOjAXtdic/9Xp06dMm7cuIwbN641LgcAAO1mMw4I2szm/GYpAACgHbRKEgEAAG8WlZKIsiQRAABAIZoIAACgEMOZAACgxOb86tW2IokAAAAKkUQAAEAJQUR5kggAAKAQSQQAAJTwitfyJBEAAEAhkggAAChREVFEOZIIAACgEEkEAACUMCeiPEkEAABQiCQCAABKSCLKk0QAAACFSCIAAKBEhSWry5JEAAAAhUgiAACghDkR5UkiAACAQiQRAABQwpSI8iQRAABAIZoIAACgEMOZAACgRKXxTGVJIgAAgEIkEQAAUMIrXsuTRAAAAIVIIgAAoIQpEeVJIgAAgEI0EQAAUKIyFW22FTF//vyMHTs2AwcOTEVFRWbOnNni+HHHHZeKiooW2yGHHNLinOXLl+eYY45J796907dv3xx//PFpaGh4Hc8IAADo8FatWpU999wzV1999auec8ghh+Tpp59u3r73ve+1OH7MMcfkoYceyuzZszNr1qzMnz8/J554YuFazIkAAIASHXVOxKGHHppDDz30Nc+pqqpKTU3NKx57+OGHc/vtt+e+++7LXnvtlSS58sorc9hhh+WrX/1qBg4cuNG1SCIAAOBNYu7cuenXr1923XXXnHzyyXn22Webjy1YsCB9+/ZtbiCSZNSoUamsrMw999xT6D6SCAAAKNGW60Q0NjamsbGxxb6qqqpUVVUVvtYhhxySI488MoMHD85jjz2Wc889N4ceemgWLFiQTp06pb6+Pv369Wvxmc6dO6e6ujr19fWF7iWJAACAdlJXV5c+ffq02Orq6l7XtY4++ugcfvjh2WOPPTJu3LjMmjUr9913X+bOndu6RUcSAQAALVS24aSIKVOmZPLkyS32vZ4U4pXsuOOO2XrrrfPoo4/moIMOSk1NTZYuXdrinHXr1mX58uWvOo/i1WgiAACgnbzeoUsb48knn8yzzz6bAQMGJElqa2vz/PPPZ+HChRkxYkSS5M4778yGDRsycuTIQtfWRAAAQImO+namhoaGPProo81fP/7443nggQdSXV2d6urqfP7zn8/48eNTU1OTxx57LGeffXaGDBmS0aNHJ0l22223HHLIITnhhBMybdq0rF27NpMmTcrRRx9d6M1MiTkRAACwWfj973+f4cOHZ/jw4UmSyZMnZ/jw4bngggvSqVOn/OlPf8rhhx+eXXbZJccff3xGjBiR3/zmNy2Sju9+97sZOnRoDjrooBx22GF597vfnWuvvbZwLZIIAAAo0ZZzIorYf//909TU9KrH77jjjrLXqK6uzowZM95wLZIIAACgEEkEAACU6KBBRIciiQAAAArRRAAAAIUYzgQAACX8lr08zwgAAChEEgEAACUqzKwuSxIBAAAUIokAAIAScojyJBEAAEAhkggAAChRaU5EWZIIAACgEEkEAACUkEOUJ4kAAAAKkUQAAEAJUyLKk0QAAACFSCIAAKCEFavLk0QAAACFSCIAAKCE37KX5xkBAACFSCIAAKCEORHlSSIAAIBCNBEAAEAhhjMBAEAJg5nKk0QAAACFSCIAAKCEidXlSSIAAIBCJBEAAFDCb9nL84wAAIBCJBEAAFDCnIjyJBEAAEAhkggAACghhyhPEgEAABQiiQAAgBKmRJQniQAAAAqRRAAAQIlKsyLKkkQAAACFSCIAAKCEORHlSSIAAIBCJBEAAFCiwpyIsiQRAABAIZIIAAAoYU5EeZIIAACgEE0EAABQiOFMAABQwmJz5UkiAACAQiQRAABQwsTq8iQRAABAIZIIAAAoIYkoTxIBAAAUIokAAIASFd7OVJYkAgAAKEQSAQAAJSoFEWVJIgAAgEIkEQAAUMKciPIkEQAAQCGSCAAAKGGdiPIkEQAAQCGSCAAAKGFORHmSCAAAoBBJBAAAlLBORHmSCAAAoBBNBAAAUIjhTAAAUMLE6vIkEQAAQCGSCChg+vXfytVXfC1HH3Nszjj73KxY8XyunXpV7l7w2yypfzp9t6zO/gcclJMmnpqevXq1d7kAOeED784JR70n2w+sTpI8/Pf6XHztbfnlb/+SJLnys0fnwJG7ZsA2fdLwUmPu/uPjOe8bP8lf/7EkSbLHLm/JmR9/b975tp2yVd8e+edTy3PdLXfl6u/Nba9vCTY5i82Vp4mAjfTQnx/Mrbf8IDvvsmvzvmVLl2bZsqX59OSzs+NOO+Xpp57KJV+8MMuWLc2XL/tGO1YL8C//s+T5nH/lT/Lo4mWpSEU+OnZkfnj5idnn6Evy8N/rc//DT+T7t92XJ55+LtV9uuezJ43JrKkTM/R9n8uGDU0Zvtt2Wbb8hXz8vBvzZP1z2WfPHXP1eR/O+g0bMu0H89v72wPaSUVTU1NTexfR2lau3tDeJfAm8+KLq3Lsh8bn7M9ekG9/a1p22XVozjj73Fc891e/vD0XnHt25t/9h3TurE+ndfSvPbW9S+BN5H/mfjnnfn1mbpy54GXHdt95YO67+dwMG3thHn/ymVf8/OWf+WCGDu6fQ//7yk1dKm9iL91/VXuX8Kp++7fn2uxe79p5yza7V2syJwI2wqUXfyHv2ne/jNznnWXPbWh4IT169tRAAB1OZWVFPjB6RHp065p7/vT4y45336JrPnb4Pnn8yWfyZP2r/yOqT88t8tzKFzdlqUAH5185UMYvb/t5Hnn4L7lxxg/Lnvv8c8/l+muvyfvHf7ANKgPYOP81ZGDm3nhGtujaOQ0vNeZDZ3wrj/y9vvn4iR94T7502rj07F6VRY/XZ8zJV2XtuvWveK199hycow4ekfefek1blQ9trtKkiLI6dBLxxBNP5BOf+MRrntPY2JiVK1e22BobG9uoQt7s6uufzmWX1uULdV9JVVXVa57b0NCQ0yadlME7DsmJJ01sowoByvvrP5Zk5NF12fdjX823fnhXvnXRsRm6Y03z8e/fdl/2+fAlGXX85fnb4mW56cufSFXXl/+ecdhOA3Lz5SfmS9f+InPufqQtvwWgg+nQTcTy5ctz4403vuY5dXV16dOnT4vta1+5pI0q5M3ukb88lOXLn82xR4/PPm/fPfu8fff84ff35Qczbso+b98969f/6zd1q1atyqmfOiHde3TPVy6/Mp27dGnnygH+19p16/P3J57J/Q8/kQuu/Gke/Ov/ZOKH928+vrJhdR5bvCy//cNj+ciZ12XXwf1zxIF7trjG0B1r8otvnpJv/+h3+fJ1d7TxdwBtq6INt81Vuw5n+ulPf/qax//+97+XvcaUKVMyefLkFvsam/wDjtax98jafO+Wn7TYd9HnPpsddhicj338k+nUqVMaGhpy6smfTJeuXfO1b0wtm1gAtLfKiopXTBqSpKKiIhWpSNcu/3t8tx1rctu1p+a7P7snF179s7YqE+jA2rWJGDduXCoqKvJaL4iqKDMmraqq6mX/aPN2JlpLjx49MmTnXVrs69atW/r07ZshO++ShoaGnHLS8Vm9enUuuvjSNKxqSMOqhiTJlltWp1OnTu1RNkCzi045PHf89qE88fRz6dVji3zo0L2y7147Z+ynpmaHt2yVo0aPyJwFD+eZ5xrylv59c8bHD85LjWtzx10PJfnXEKbbrj01v/rdw7nipjvTf6t/rYGzfkNTnnmuoT2/Ndh0NueIoI20axMxYMCATJ06NUccccQrHn/ggQcyYsSINq4KNt6ih/+SPz/4pyTJ+983usWxn/ziVxn4lre0R1kAzbap7pnrv/Cx1GzdOysaVufPf/ufjP3U1Nx5zyMZsE2fvGv4Tpn0kf2zZe/uWfrsC7nrD4/mgOMuy7L/v0F4/6jh6VfdKx953zvykfe9o/m6/3zq2Qwd87n2+raAdtau60Qcfvjhedvb3paLLrroFY//8Y9/zPDhw7NhQ7FkQRIBvNlYJwJ4s+nI60Tc89iKNrvXyJ36tNm9WlO7JhFnnXVWVq1a9arHhwwZkl//+tdtWBEAAFBOuzYR73nPe17zeI8ePbLffvu1UTUAAJBYJqK8Dv2KVwAAoOPRRAAAQImOuk7E/PnzM3bs2AwcODAVFRWZOXNmi+NNTU254IILMmDAgHTr1i2jRo3K3/72txbnLF++PMccc0x69+6dvn375vjjj09DQ/E3rWkiAABgM7Bq1arsueeeufrqq1/x+KWXXporrrgi06ZNyz333JMePXpk9OjRWb16dfM5xxxzTB566KHMnj07s2bNyvz583PiiScWrqVd3860qXg7E/Bm4+1MwJtNR347032Pt93bmfYe/PrezlRRUZFbb70148aNS/KvFGLgwIE544wzcuaZZyZJVqxYkf79+2f69Ok5+uij8/DDD2fYsGG57777stdeeyVJbr/99hx22GF58sknM3DgwI2+vyQCAAA2c48//njq6+szatSo5n19+vTJyJEjs2DBgiTJggUL0rdv3+YGIklGjRqVysrK3HPPPYXu165vZwIAgP9kjY2NaWxsbLGvqqoqVVVVha5TX1+fJOnfv3+L/f37928+Vl9fn379+rU43rlz51RXVzefs7EkEQAAUKKiDf+rq6tLnz59Wmx1dXXt/QjKkkQAAEA7mTJlSiZPntxiX9EUIklqamqSJEuWLMmAAQOa9y9ZsiRve9vbms9ZunRpi8+tW7cuy5cvb/78xpJEAABAiYqKttuqqqrSu3fvFtvraSIGDx6cmpqazJkzp3nfypUrc88996S2tjZJUltbm+effz4LFy5sPufOO+/Mhg0bMnLkyEL3k0QAAMBmoKGhIY8++mjz148//ngeeOCBVFdXZ9CgQTnttNPyxS9+MTvvvHMGDx6c888/PwMHDmx+g9Nuu+2WQw45JCeccEKmTZuWtWvXZtKkSTn66KMLvZkp0UQAAEALRReBayu///3vc8ABBzR//e9hUBMmTMj06dNz9tlnZ9WqVTnxxBPz/PPP593vfnduv/32bLHFFs2f+e53v5tJkybloIMOSmVlZcaPH58rrriicC3WiQDYDFgnAniz6cjrRPzhHyvb7F5v36F3m92rNUkiAACgVEeNIjoQE6sBAIBCJBEAAFCiQhRRliQCAAAoRBIBAAAlKgQRZUkiAACAQiQRAABQQhBRniQCAAAoRBIBAAClRBFlSSIAAIBCJBEAAFDCOhHlSSIAAIBCNBEAAEAhhjMBAEAJi82VJ4kAAAAKkUQAAEAJQUR5kggAAKAQSQQAAJQSRZQliQAAAAqRRAAAQAmLzZUniQAAAAqRRAAAQAnrRJQniQAAAAqRRAAAQAlBRHmSCAAAoBBJBAAAlBJFlCWJAAAACpFEAABACetElCeJAAAACpFEAABACetElCeJAAAACtFEAAAAhRjOBAAAJYxmKk8SAQAAFCKJAACAUqKIsiQRAABAIZIIAAAoYbG58iQRAABAIZIIAAAoYbG58iQRAABAIZIIAAAoIYgoTxIBAAAUIokAAIBSooiyJBEAAEAhkggAAChhnYjyJBEAAEAhkggAAChhnYjyJBEAAEAhkggAACghiChPEgEAABQiiQAAgFKiiLIkEQAAQCGaCAAAoBDDmQAAoITF5sqTRAAAAIVIIgAAoITF5sqTRAAAAIVIIgAAoIQgojxJBAAAUIgkAgAASpgTUZ4kAgAAKEQSAQAALYgiypFEAAAAhUgiAACghDkR5UkiAACAQiQRAABQQhBRniQCAAAoRBIBAAAlzIkoTxIBAAAUIokAAIASFWZFlCWJAAAACtFEAAAAhRjOBAAApYxmKksSAQAAFCKJAACAEoKI8iQRAABAIZIIAAAoYbG58iQRAABAIZIIAAAoYbG58iQRAACwGbjwwgtTUVHRYhs6dGjz8dWrV2fixInZaqut0rNnz4wfPz5LlizZJLVoIgAAoFRFG24F/dd//Veefvrp5u2uu+5qPnb66afnZz/7WX74wx9m3rx5eeqpp3LkkUcWv8lGMJwJAAA2E507d05NTc3L9q9YsSLXX399ZsyYkQMPPDBJcsMNN2S33XbL3XffnX322adV65BEAABAibYMIhobG7Ny5coWW2Nj46vW9re//S0DBw7MjjvumGOOOSaLFy9OkixcuDBr167NqFGjms8dOnRoBg0alAULFrTOgymhiQAAgHZSV1eXPn36tNjq6upe8dyRI0dm+vTpuf3223PNNdfk8ccfz3ve85688MILqa+vT9euXdO3b98Wn+nfv3/q6+tbvW7DmQAAoERbrhMxZcqUTJ48ucW+qqqqVzz30EMPbf7zW9/61owcOTLbb799br755nTr1m2T1vl/SSIAAKCdVFVVpXfv3i22V2si/q++fftml112yaOPPpqampqsWbMmzz//fItzlixZ8opzKN4oTQQAAJSoaMP/3oiGhoY89thjGTBgQEaMGJEuXbpkzpw5zccXLVqUxYsXp7a29o0+kpcxnAkAADYDZ555ZsaOHZvtt98+Tz31VD73uc+lU6dO+fCHP5w+ffrk+OOPz+TJk1NdXZ3evXvnlFNOSW1tbau/mSnRRAAAQAttOSeiiCeffDIf/vCH8+yzz2abbbbJu9/97tx9993ZZpttkiSXX355KisrM378+DQ2Nmb06NGZOnXqJqmloqmpqWmTXLkdrVy9ob1LAGhV/WtPbe8SAFrVS/df1d4lvKrnXlzfZvfasnunNrtXazInAgAAKEQTAQAAFKKJAAAACjGxGgAASnTUidUdiSQCAAAoRBIBAAAl3ugicP8JJBEAAEAhkggAAChhTkR5kggAAKAQSQQAAJQQRJQniQAAAAqRRAAAQClRRFmSCAAAoBBJBAAAlLBORHmSCAAAoBBJBAAAlLBORHmSCAAAoBBJBAAAlBBElCeJAAAACpFEAABAKVFEWZIIAACgEE0EAABQiOFMAABQwmJz5UkiAACAQiQRAABQwmJz5UkiAACAQiqampqa2rsI2Bw1Njamrq4uU6ZMSVVVVXuXA/CG+bkGbCxNBLxOK1euTJ8+fbJixYr07t27vcsBeMP8XAM2luFMAABAIZoIAACgEE0EAABQiCYCXqeqqqp87nOfM/kQeNPwcw3YWCZWAwAAhUgiAACAQjQRAABAIZoIAACgEE0EAABQiCYCXqerr746O+ywQ7bYYouMHDky9957b3uXBPC6zJ8/P2PHjs3AgQNTUVGRmTNntndJQAeniYDX4Qc/+EEmT56cz33uc/nDH/6QPffcM6NHj87SpUvbuzSAwlatWpU999wzV199dXuXAmwmvOIVXoeRI0dm7733zlVXXZUk2bBhQ7bbbruccsop+cxnPtPO1QG8fhUVFbn11lszbty49i4F6MAkEVDQmjVrsnDhwowaNap5X2VlZUaNGpUFCxa0Y2UAAG1DEwEFPfPMM1m/fn369+/fYn///v1TX1/fTlUBALQdTQQAAFCIJgIK2nrrrdOpU6csWbKkxf4lS5akpqamnaoCAGg7mggoqGvXrhkxYkTmzJnTvG/Dhg2ZM2dOamtr27EyAIC20bm9C4DN0eTJkzNhwoTstddeecc73pGvf/3rWbVqVT7+8Y+3d2kAhTU0NOTRRx9t/vrxxx/PAw88kOrq6gwaNKgdKwM6Kq94hdfpqquuyle+8pXU19fnbW97W6644oqMHDmyvcsCKGzu3Lk54IADXrZ/woQJmT59etsXBHR4mggAAKAQcyIAAIBCNBEAAEAhmggAAKAQTQQAAFCIJgIAAChEEwEAABSiiQAAAArRRAB0MMcdd1zGjRvX/PX++++f0047rc3rmDt3bioqKvL888+3+b0B6Ng0EQAb6bjjjktFRUUqKirStWvXDBkyJBdddFHWrVu3Se/74x//OF/4whc26lz/8AegLXRu7wIANieHHHJIbrjhhjQ2NuYXv/hFJk6cmC5dumTKlCktzluzZk26du3aKvesrq5ulesAQGuRRAAUUFVVlZqammy//fY5+eSTM2rUqPz0pz9tHoL0pS99KQMHDsyuu+6aJHniiSfywQ9+MH379k11dXWOOOKI/OMf/2i+3vr16zN58uT07ds3W221Vc4+++w0NTW1uOf/Hc7U2NiYc845J9ttt12qqqoyZMiQXH/99fnHP/6RAw44IEmy5ZZbpqKiIscdd1ySZMOGDamrq8vgwYPTrVu37Lnnnrnlllta3OcXv/hFdtlll3Tr1i0HHHBAizoBoJQmAuAN6NatW9asWZMkmTNnThYtWpTZs2dn1qxZWbt2bUaPHp1evXrlN7/5TX7729+mZ8+eOeSQQ5o/c9lll2X69On59re/nbvuuivLly/Prbfe+pr3/NjHPpbvfe97ueKKK/Lwww/nm9/8Znr27JntttsuP/rRj5IkixYtytNPP51vfOMbSZK6urr8v//3/zJt2rQ89NBDOf300/PRj3408+bNS/KvZufII4/M2LFj88ADD+STn/xkPvOZz2yqxwbAZs5wJoDXoampKXPmzMkdd9yRU045JcuWLUuPHj1y3XXXNQ9juummm7Jhw4Zcd911qaioSJLccMMN6du3b+bOnZuDDz44X//61zNlypQceeSRSZJp06bljjvueNX7/vWvf83NN9+c2bNnZ9SoUUmSHXfcsfn4v4c+9evXL3379k3yr+Ti4osvzq9+9avU1tY2f+auu+7KN7/5zey333655pprstNOO+Wyyy5Lkuy666558MEH8+Uvf7kVnxoAbxaaCIACZs2alZ49e2bt2rXZsGFDPvKRj+TCCy/MxIkTs8cee7SYB/HHP/4xjz76aHr16tXiGqtXr85jjz2WFStW5Omnn87IkSObj3Xu3Dl77bXXy4Y0/dsDDzyQTp06Zb/99tvomh999NG8+OKLee9739ti/5o1azJ8+PAkycMPP9yijiTNDQcA/F+aCIACDjjggFxzzTXp2rVrBg4cmM6d//fHaI8ePVqc29DQkBEjRuS73/3uy66zzTbbvK77d+vWrfBnGhoakiQ///nP85a3vKXFsaqqqtdVBwD/2TQRAAX06NEjQ4YM2ahz3/72t+cHP/hB+vXrl969e7/iOQMGDMg999yTfffdN0mybt26LFy4MG9/+9tf8fw99tgjGzZsyLx585qHM5X6dxKyfv365n3Dhg1LVVVVFi9e/KoJxm677Zaf/vSnLfbdfffd5b9JAP4jmVgNsIkcc8wx2XrrrXPEEUfkN7/5TR5//PHMnTs3p556ap588skkyac//elccsklmTlzZh555JF86lOfes01HnbYYYdMmDAhn/jEJzJz5szma958881Jku233z4VFRWZNWtWli1bloaGhvTq1StnnnlmTj/99Nx444157LHH8oc//CFXXnllbrzxxiTJSSedlL/97W8566yzsmjRosyYMSPTp0/f1I8IgM2UJgJgE+nevXvmz5+fQYMG5cgjj8xuu+2W448/PqtXr25OJs4444wce+yxmTBhQmpra9OrV6+8//3vf83rXnPNNTnqqKPyqU99KkOHDs0JJ5yQVatWJUne8pa35POf/3w+85nPpH///pk0aVKS5Atf+ELOP//81NXVZbfddsshhxySn//85xk8eHCSZNCgQfnRj36UmTNnZs8998y0adNy8cUXb8KnA8DmrKLp1WbvAQAAvAJJBAAAUIgmAgAAKEQTAQAAFKKJAAAACtFEAAAAhWgiAACAQjQRAABAIZoIAACgEE0EAABQiCYCAAAoRBMBAAAUookAAAAK+f8A81bCWIe30fMAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.60      0.56        80\n",
            "           1       0.91      0.89      0.90       374\n",
            "\n",
            "    accuracy                           0.84       454\n",
            "   macro avg       0.72      0.74      0.73       454\n",
            "weighted avg       0.85      0.84      0.84       454\n",
            "\n",
            "Macro F1 Score on Validation set: 0.732217439821457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discriminative Model** **BERT**"
      ],
      "metadata": {
        "id": "lnmM72F7evig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.iloc[index]['text']\n",
        "        sentiment = self.data.iloc[index]['sentiment_encoded']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(sentiment, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "Aghxen_se-Vc"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data\n",
        "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "val_df['text'] = val_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Parameters\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = SentimentDataset(train_df, tokenizer, MAX_LEN)\n",
        "val_dataset = SentimentDataset(val_df, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize DistilBERT model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=2)\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in tqdm(data_loader, desc=\"Training\"):\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxVHnq-XfMn3",
        "outputId": "c613b2bf-3078-42fe-dccd-8662ed4a2f0e"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses), all_preds, all_labels\n",
        "\n",
        "# Training loop\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss, preds, labels = eval_model(\n",
        "        model,\n",
        "        val_loader,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, 'best_model_state.bin'))\n",
        "        best_accuracy = val_acc\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(os.path.join(model_dir, 'best_model_state.bin')))\n",
        "\n",
        "# Evaluate the best model\n",
        "val_acc, val_loss, preds, labels = eval_model(model, val_loader, device)\n",
        "\n",
        "# Classification report\n",
        "class_names = ['negative', 'positive']\n",
        "report = classification_report(labels, preds, target_names=class_names)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Macro F1 Score\n",
        "score = f1_score(labels, preds, average='macro')\n",
        "print('Macro F1 Score on Validation set:', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6x4CLqDf1M1",
        "outputId": "c2ffbb3f-bff0-4f14-9021-af3b8942c98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|█████████▉| 916/920 [44:16<00:11,  2.76s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Other Method/model End"
      ],
      "metadata": {
        "id": "7yMswIeAwYIf"
      }
    }
  ]
}